---
title: "Weight lifting exercise-PML course project"
output:
  html_document:
    smart: no
  pdf_document: default
---

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now
possible to collect a large amount of data about personal activity relatively
inexpensively. These type of devices are part of the quantified self movement
- a group of enthusiasts who take measurements about themselves regularly to
improve their health, to find patterns in their behavior, or because they are
tech geeks. One thing that people regularly do is quantify how much of a
particular activity they do, but they rarely quantify how well they do it. In
this project, we use data from accelerometers on the belt,
forearm, arm, and dumbell of 6 participants.  
They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 
  
More information is available from the website here: 
http://groupware.les.inf.puc-rio.br/har
(see the section on the Weight Lifting Exercise Dataset). 

# Activity data

The training data for this project are available here:   
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:  
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

```{r cache=TRUE}
library(RCurl)
train_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

train_data <- read.csv(text=getURL(train_url), na.strings=c("", "NA"))
test_data <- read.csv(text=getURL(test_url), na.strings=c("", "NA"))
```
  
# Analysis objective
The main goal of this project is to predict the manner in which they did the exercise using the "classe" variable in the training set as the response. However, I could use any of the other variables to predict with.   

# Report format
To create a report describing how the I built my model, how I used cross validation, the expected ouput of sample error, and why I made the choices you that I took.   
I will also use the prediction model to predict 20 different test cases. 
# Starting analysis
The first column of the data is just index. We remove it from training data
frame.
```{r}
train_data$X <- NULL
```

Similarly the user and time information should not have any effect on
whether barbell lifts are performed correctly or not.

```{r}
cols_to_remove <- c("user_name", "raw_timestamp_part_1",
                    "raw_timestamp_part_2", "cvtd_timestamp")
for (col in cols_to_remove) {
    train_data[, col] <- NULL
}
```

Many columns in the dataset have mostly missing values. We remove
features from the training and testing data that have too many missing
values, where imputing is not an option.

```{r}
NAs <- apply(train_data,2,function(x) {sum(is.na(x))})
train_data <- train_data[,which(NAs == 0)]
```

We also exclude columns containing constant values (i.e. zero
variance predictors) or they contains few unique values relative to the number of samples and the ratio of frequency of the most common value to the
frequency of second most common value is large.

```{r,message=FALSE}
library(caret)
unq<- nearZeroVar(train_data)
unq.tst<- nearZeroVar(test_data)
```
Although test data has got more columns  with near zero variance values than train data we only remove from the the test data columns matching those in the training data.

```{r, message=FALSE}
train_data <- train_data[-unq]
test_data <- test_data[-unq]
```

The following are therefore the selected set of predictors to be used for classification and then used for model validation.

```{r}
names(train_data)
```

# Classification model

Using caret library we develop a random forest model classifier to predict the action class. To validate the model we use a cross-validation procedure to measure the accuracy of the model using a  10-fold and split th train data to 80% training and 20% testing.

```{r cache=TRUE}
library(randomForest)
set.seed(1)
obs <- c()
preds <- c()
for(i in 1:10) {
    intrain = sample(1:dim(train_data)[1], size=dim(train_data)[1] * 0.8, replace=F)
    train_cross = train_data[intrain,]
    test_cross = train_data[-intrain,]
    rf <- randomForest(classe ~ ., data=train_cross)
    obs <- c(obs, test_cross$classe)
    preds <- c(preds, predict(rf, test_cross))
}
```

The confusion matrix for predictions on cross validation folds is given below.

```{r}
conf_mat <- confusionMatrix(table(preds, obs))
conf_mat$table
```

The proposed model seems classifying well enough. The accuracy is 
`r conf_mat$overall[[1]] * 100`% and it misclassifies only few instances. 
Finally, we train the random forest
with whole dataset so that the classifier can be used to predict the class of
an action, given the set of activity measurements.

```{r cache=TRUE}
model <- randomForest(classe ~ ., data=train_data)
```
## Predicting test data

We can apply the randomForest model to the 20 given test set for the predictions. The results were all correct.

```{r cache = TRUE}
# apply random forest model to test set
pds<-predict(model, test_data)
```

# Conclusion
We see the cross-validated model classifying with high accuracy using the training set. But we are seeing a lower accuracy when the overall classification model is applied to the independent test data.

```{r, message=FALSE}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
      filename = paste0("pred",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(pds)
```

#Appendix: Data visualization
```{r, message=FALSE}
library(rpart.plot)
library(rpart)
treeModel <- rpart(classe ~ ., data=train_data, method="class")
prp(treeModel) 
```